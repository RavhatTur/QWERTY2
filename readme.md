1. Понятие проактивного поиска угроз информационной безопасности (cyber threat hunting). Определение, задачи, основные средства. Общая последовательность этапов проактивного поиска угроз информационной безопасности. 

—
Проактивный поиск угроз (cyber threat hunting) — это процесс активного поиска киберугроз в сети до того, как они будут выявлены автоматическими средствами обнаружения:

Задачи:
Выявление скрытых угроз.
Анализ потенциальных векторов атак.
Повышение уровня защиты за счет улучшения механизмов обнаружения.

Основные средства:

SIEM (Security Information and Event Management) - это система, которая собирает, анализирует и управляет информацией о безопасности в реальном времени. 
EDR (Endpoint Detection and Response) - это технология, которая обеспечивает непрерывный мониторинг и анализ активности на конечных точках для обнаружения и реагирования на угрозы безопасности. 
Инструменты анализа сетевого трафика.

Этапы:
Определение гипотезы: Формулирование предположений о возможных угрозах.
Сбор данных: Получение информации из различных источников (логов, сетевых событий).
Анализ данных: Проверка гипотез путем анализа собранных данных.
Действие: Реагирование на обнаруженные угрозы.
—

2. Язык программирования R. Основные типы данных. Перевод из одних типов данных в другие.

—
Язык программирования R - это язык и среда для статистического анализа и визуализации данных. Основные типы данных в R включают числа (integer, numeric), логические значения (logical), символы (character) и факторы (factor). Перевод из одного типа данных в другой можно осуществить с помощью функций, таких как as.integer(), as.character(), as.logical() и т. д.

— 

3. Язык программирования R. Функции, оператор конвейера. Примеры. 

—
Язык программирования R - это язык и среда для статистического анализа и визуализации данных. Конвейер в пакете dplyr - это последовательное применение функций к данным с помощью оператора %>% (pipe). Примеры использования конвейера в пакете dplyr включают комбинирование операций фильтрации, сортировки и группировки данных.
—

4. Язык программирования R. Аналитическая экосистема Tidyverse, общие подходы, состав. 

—
Язык программирования R - это язык и среда для статистического анализа и визуализации данных. Аналитическая экосистема Tidyverse - это набор пакетов R, которые обеспечивают согласованный и удобный для использования набор функций для работы с данными. Основные подходы в Tidyverse включают работу с данными в формате dataframe, использование оператора конвейера %>% (pipe) для последовательного применения функций.. Состав Tidyverse включает пакеты dplyr, tidyr, ggplot2, readr.
—

5. Язык программирования R. Применение функции dplyr::mutate() 

—
Язык программирования R - это язык и среда для статистического анализа и визуализации данных. mutate() - это функция из пакета dplyr, которая добавляет новые столбцы к существующему датафрейму или изменяет значения существующих переменных. Функция mutate() принимает в качестве аргументов датафрейм и спецификацию новых переменных.
—

6. Язык программирования R. Применение функции dplyr::filter() 

—
Язык программирования R - это язык и среда для статистического анализа и визуализации данных. filter() - это функция из пакета dplyr, которая фильтрует строки датафрейма на основе заданных условий. Функция filter() принимает в качестве аргументов датафрейм и условия фильтрации, указанные с помощью операторов сравнения (например, ==, >, <) и логических операторов (например, & - логическое "и", | - логическое "или").
—

7. Язык программирования R. Применение функции dplyr::select() 

—
Язык программирования R - это язык и среда для статистического анализа и визуализации данных. select() - это функция из пакета dplyr, которая выбирает определенные переменные (столбцы) из датафрейма. Функция select() принимает в качестве аргументов датафрейм и список переменных, которые нужно выбрать. 
—

8. Язык программирования R. Импорт данных, чтение данных из файла. 

—
Язык программирования R - это язык и среда для статистического анализа и визуализации данных. Одним из наиболее популярных пакетов является readr. Вот примеры кода: 
# Чтение данных из CSV файла
data <- read_csv("file.csv")

# Чтение данных из Excel файла
data <- read_excel("file.xlsx")

# Чтение данных из базы данных SQLite
data <- read_sqlite("database.sqlite", "table_name")
—

9. Язык программирования R. Сохранение рабочей области, запись данных в файл.

—
Язык программирования R - это язык и среда для статистического анализа и визуализации данных.
# Сохранение рабочей области в файл
save.image("workspace.RData")

# Запись данных в CSV файл
write_csv(data, "file.csv")

# Запись данных в Excel файл
write_excel(data, "file.xlsx")

# Запись данных в базу данных SQLite
write_sqlite(data, "database.sqlite", "table_name")
 —

10. Язык программирования R. Визуализация данных: основные средства, примеры.

—
Язык программирования R - это язык и среда для статистического анализа и визуализации данных.
Для визуализации данных в R можно использовать пакет  ggplot2
Пример кода:
ggplot(data, aes(x = category, y = value)) +
  geom_bar(stat = "identity")
Функция ggplot(), которая создает новый объект графика. Внутри функции мы указываем данные, которые будут использоваться для построения графика (data), а также задаем эстетику (aes - aesthetics), то есть указываем, какие переменные будут использоваться для определения положения столбцов по оси x (x = category) и высоты столбцов по оси y (y = value).

geom_bar(stat = "identity") - это слой графика, который определяет тип геометрии, в данном случае столбчатую диаграмму. Параметр stat = "identity" указывает, что высота каждого столбца будет определяться непосредственно значениями переменной value в данных.

+ - это оператор, который используется для добавления слоев на график.
—

11. Понятие этапа очистки данных, нормализация данных, “аккуратные данные”. 

—
Этап очистки данных включает в себя процесс преобразования данных в "аккуратный" формат, устранение ошибок, пропусков и выбросов, а также приведение данных к соответствующим форматам. Нормализация данных - это процесс приведения данных к общему масштабу или диапазону, чтобы они были сопоставимы. "Аккуратные данные" - это данные, которые легко анализировать и использовать без предварительной обработки. Это включает в себя правильное представление данных, отсутствие дубликатов, непротиворечивое представление переменных и обработку пропущенных значений.
—

12. Особенности применения средства Zeek (Bro) для сбора информации от сетевом трафике. Примеры использования. 

—
Zeek (ранее известный как Bro) - это мощный и гибкий сетевой анализатор, который используется для мониторинга сетевого трафика и обеспечения безопасности. Основные особенности Zeek включают:

Гибкость и настраиваемость: Zeek предоставляет скриптовый язык, который позволяет настраивать и расширять его функциональность под конкретные потребности.
Анализ протоколов: Zeek поддерживает глубокий анализ множества сетевых протоколов (HTTP, DNS, FTP, SMTP и т.д.).
Журналирование: Zeek генерирует детализированные журналы (логи) сетевой активности, которые можно использовать для последующего анализа.
Интеграция с другими инструментами: Zeek можно интегрировать с различными системами и инструментами для безопасности и мониторинга, такими как SIEM-системы.
Примеры использования:

Обнаружение вторжений: Использование Zeek для выявления аномальной сетевой активности, которая может свидетельствовать о кибератаках.
Анализ трафика в реальном времени: Мониторинг и анализ сетевого трафика в реальном времени для оперативного обнаружения инцидентов безопасности.
Сбор доказательств: Журналы Zeek могут быть использованы для расследования инцидентов безопасности, предоставляя детализированные данные о сетевой активности.

—

13. Пакет ggplot2. Концепция визуализации данных и базовые примитивы визуализации функции ggplot2::ggplot2 

—
ggplot2 - это мощный пакет для визуализации данных в языке R, основанный на грамматике графиков (The Grammar of Graphics). Основные концепции ggplot2 включают:

Грамматика графиков: Позволяет описывать и строить графики через набор элементов (эстетики, геометрические объекты, координатные системы и т.д.).
Базовые примитивы визуализации:
ggplot(): Функция для создания объекта графика.
aes(): Функция для установки эстетик (например, x, y координаты, цвет, размер).
geom_*(): Геометрические функции для добавления различных типов графиков (точечные графики, линии, гистограммы и т.д.).
—

14. Понятие больших данных. Файловые форматы хранения данных. 

—
Большие данные (Big Data) - это термин, используемый для описания объема данных, которые слишком велики для традиционных методов обработки данных. Большие данные характеризуются высокой скоростью генерации, разнообразием и сложностью данных.
Файловые форматы хранения данных для больших данных включают Parquet, JSON. Эти форматы оптимизированы для эффективного хранения и обработки больших объемов данных, обеспечивая компактное представление, разделение на блоки, сжатие и быстрый доступ к данным.
—

15. Инструменты командной строки операционной системы Linux для поиска информации в текстовых файлах. 

—
В Linux существует множество команд для поиска информации в текстовых файлах, включая:

grep: Поиск строк, соответствующих заданному шаблону.
grep 'pattern' filename

awk: Мощный инструмент для обработки и анализа текстовых файлов.
awk '/pattern/ {print $0}' filename

sed: Стримовый редактор для поиска и замены текста.
sed 's/old/new/g' filename

find: Поиск файлов и выполнение команд над ними.
find /path -name "filename" -exec grep 'pattern' {} \;

—

16. Работа с большими данными в R. Подход lazy-loading. Пакет vroom. 

—
В R для работы с большими данными используется подход "ленивой загрузки" (lazy-loading). Это означает, что данные загружаются по мере необходимости, а не все сразу. Такой подход позволяет экономить использование оперативной памяти и обрабатывать данные, которые не помещаются полностью в память.
Пакет vroom является инструментом для эффективной загрузки больших данных в R. Он использует подход ленивой загрузки и может загружать данные из различных форматов, таких как CSV, TSV и файлы фиксированной ширины. Пакет vroom предоставляет функции для быстрой загрузки и обработки данных, а также поддерживает параллельную обработку.
—

17. Программная платформа работы с большими данными Apache Arrow: концепция организации вычислений. 

—
Apache Arrow - это программная платформа для эффективной обработки больших данных. Она предоставляет средства для организации вычислений и обмена данными между различными системами и языками программирования.
Концепция организации вычислений в Apache Arrow включает использование колоночного формата хранения данных, векторизованных операций и оптимизации памяти. Колоночное хранение позволяет эффективно сжимать и обрабатывать данные, векторизованные операции позволяют выполнять операции над данными сразу для целых столбцов, а оптимизация памяти уменьшает накладные расходы на хранение и обработку данных.
—


18. Программная платформа работы с большими данными DuckDB: концепция организации вычислений. Работа с DuckDB из R. Организация подключения.

—
DuckDB - это программная платформа для работы с большими данными, которая обеспечивает высокую производительность и эффективность выполнения запросов на больших объемах данных. Основная концепция DuckDB состоит в том, что данные хранятся в памяти и разделены на блоки, которые обрабатываются параллельно, что позволяет достичь высокой скорости выполнения запросов.

Для работы с DuckDB из R можно использовать пакет duckdb, который предоставляет возможности подключения к базе данных DuckDB и выполнения запросов на R. Для подключения к DuckDB из R необходимо установить пакет duckdb и указать параметры подключения, такие как имя базы данных, хост и порт.
—

19. Работа с большими данными. Колоночные форматы хранения данных. SIMD.

—
Колоночные форматы хранения данных - это форматы, в которых данные хранятся по столбцам, а не по строкам. Это позволяет сжимать и обрабатывать данные более эффективно, особенно для аналитических запросов, которые часто оперируют с отдельными столбцами.
SIMD (Single Instruction, Multiple Data) - это технология, которая позволяет одному процессору выполнять одну инструкцию над несколькими данными одновременно. SIMD используется для ускорения операций с данными, таких как сжатие, фильтрация, агрегация и т.д. SIMD может быть эффективно применен при работе с колоночными форматами хранения данных, так как операции выполняются над целыми столбцами.
—
 
20. Работа с большими данными. OLAP СУБД: концепция организации вычислений, примеры программных продуктов. 

—
OLAP (Online Analytical Processing) СУБД (системы управления базами данных) - это специализированные программные продукты для обработки и анализа больших объемов данных. Они предназначены для выполнения аналитических запросов, а не транзакционной обработки данных.
Концепция организации вычислений в OLAP СУБД включает использование многомерных структур данных, таких как кубы и срезы, агрегирование данных, предварительное вычисление и кэширование результатов запросов.
Примеры программных продуктов OLAP СУБД включают Oracle Essbase, Microsoft SQL Server Analysis Services.. Эти системы предоставляют инструменты для моделирования данных, создания отчетов, анализа и визуализации больших объемов данных.
—

21. Работа с Clickhouse из R. Организация подключения к СУБД.

—
ClickHouse - это колоночная СУБД (система управления базами данных), разработанная для анализа больших объемов данных. Она обладает высокой производительностью и эффективностью при работе с большими объемами данных.

Для работы с ClickHouse из R можно использовать пакет ClickHouseHTTP, который предоставляет возможности подключения к базе данных ClickHouse и выполнения запросов на R. Для подключения к ClickHouse из R необходимо установить пакет и указать параметры подключения, такие как имя базы данных, хост, порт, имя пользователя и пароль.

—

22. Yandex Query: назначение, основные возможности, основные концепции работы, использование. 

—-
Yandex Query - это сервис от Яндекса, предназначенный для выполнения сложных аналитических запросов к большим объемам данных. Он позволяет работать с данными, хранящимися в различных источниках, таких как ClickHouse, PostgreSQL, MySQL и другие.

Основные возможности Yandex Query включают выполнение сложных аналитических запросов, создание временных таблиц, использование различных функций агрегации и группировки данных. Он также поддерживает работу с различными форматами данных, такими как CSV, JSON, Parquet и другими.

Основные концепции работы Yandex Query включают использование SQL-подобного языка запросов, возможность создания пользовательских функций и использование распределенных вычислений для обработки больших объемов данных.

Использование Yandex Query включает создание запросов для анализа данных, выполнение запросов и получение результатов в нужном формате данных.
—-

23. Yandex Datalens: назначение, основные возможности, использование

—
Yandex Datalens - это сервис от Яндекса, предназначенный для визуализации и анализа данных. Он позволяет создавать интерактивные дашборды и отчеты на основе данных из различных источников.

Основные возможности Yandex Datalens включают создание интерактивных графиков, диаграмм, таблиц и других элементов визуализации данных. Он также поддерживает фильтрацию данных, сортировку, группировку и агрегацию данных.

Использование Yandex Datalens включает подключение к источникам данных, создание и настройку элементов визуализации, создание дашбордов и отчетов, а также публикацию их для доступа другим пользователям.

—

24. Yandex Managed Service for Clickhouse: назначение, основные возможности, использование.

—
Yandex Managed Service for ClickHouse - это сервис от Яндекса, предназначенный для управления и обслуживания баз данных ClickHouse. Он предоставляет возможности автоматического масштабирования, резервного копирования, мониторинга и обновления баз данных ClickHouse.

Основные возможности Yandex Managed Service for ClickHouse включают автоматическое масштабирование баз данных ClickHouse в зависимости от нагрузки, автоматическое резервное копирование данных для обеспечения их сохранности, мониторинг производительности и доступности баз данных ClickHouse, а также автоматическое обновление версии ClickHouse.

Использование Yandex Managed Service for ClickHouse включает создание и настройку баз данных ClickHouse, мониторинг и управление производительностью баз данных, резервное копирование и восстановление данных, а также обновление версии ClickHouse для получения новых функций и улучшений.
—

25. Определение искусственного интеллекта. Понятие рациональности искусственного интеллекта. 

—
Искусственный интеллект (ИИ) - это область компьютерной науки, направленная на создание систем, способных выполнять задачи, требующие человеческого интеллекта, такие как обучение, понимание языка, распознавание образов и принятие решений.

Рациональность ИИ:

Теоретическая рациональность: ИИ принимает решения, которые максимизируют ожидаемую полезность на основе заданной модели мира и целей.
Практическая рациональность: В реальных условиях ИИ принимает решения, которые являются оптимальными с учетом ограничений по времени и ресурсам.
—

26. Характеристики операционной (проблемной) среды для интеллектуальных агентов. Примеры. 

—
Операционная среда для интеллектуальных агентов характеризуется следующими параметрами:

Наблюдаемость: Полная (все состояния известны) или частичная (только часть информации доступна).
Детерминированность: Детерминированная (действия приводят к предсказуемым результатам) или стохастическая (результаты действий случайны).
Эпизодичность: Эпизодическая (действия независимы) или последовательная (действия влияют на последующие состояния).
Динамичность: Статическая (не изменяется) или динамическая (может изменяться).
Множественность агентов: Одноагентная или многоагентная.
Примеры:

Шахматы: Полностью наблюдаемая, детерминированная, последовательная, статическая, одноагентная (с точки зрения одного игрока).
Автономные автомобили: Частично наблюдаемая, стохастическая, последовательная, динамическая, многоагентная.
—

27. Определение машинного обучения. Основные виды машинного обучения. 

—
Машинное обучение (МО) - это раздел ИИ, посвященный разработке алгоритмов и моделей, которые позволяют компьютерам обучаться на данных и делать прогнозы или принимать решения.

Основные виды МО:

Обучение с учителем (Supervised Learning): Обучение на размеченных данных (например, классификация, регрессия).
Обучение без учителя (Unsupervised Learning): Обучение на неразмеченных данных (например, кластеризация, ассоциативные правила).
Обучение с подкреплением (Reinforcement Learning): Обучение через взаимодействие с окружающей средой и получение обратной связи в виде вознаграждений или наказаний.
—

28. Процесс обучения модели машинного обучения. Требования к обучающей выборке. Типовые проблемы в данных, затрудняющие применение методов машинного обучения

—
Процесс обучения модели машинного обучения включает следующие шаги:

Сбор и подготовка данных: сбор данных из различных источников, очистка и предобработка данных, преобразование данных в удобный формат.
Выбор модели: выбор подходящей модели машинного обучения в зависимости от задачи и типа данных.
Разделение данных: разделение данных на обучающую и тестовую выборки для оценки производительности модели.
Обучение модели: обучение модели на обучающей выборке с использованием выбранного алгоритма машинного обучения.
Оценка производительности модели: оценка производительности модели на тестовой выборке с использованием метрик оценки качества модели.
Требования к обучающей выборке включают следующие аспекты:

Репрезентативность: выборка должна быть репрезентативной для общей популяции данных, чтобы модель могла обучаться на различных случаях и ситуациях.
Разнообразие: выборка должна содержать разнообразные примеры, чтобы модель могла обучаться на различных вариациях данных и быть устойчивой к шуму и выбросам.
Полнота: выборка должна содержать достаточное количество примеров, чтобы модель могла обучиться на всех возможных случаях и сделать обобщенные выводы.
Типовые проблемы в данных, затрудняющие применение методов машинного обучения, включают:

Недостаточность данных: если выборка содержит недостаточное количество примеров, модель может быть недообучена и не сможет давать точные предсказания.
Нерепрезентативность данных: если выборка не является репрезентативной для общей популяции данных, модель может быть смещена и давать неточные предсказания.

—

30. Проблема переобучения моделей. Основные способы контроля переобучения. 

—
Проблема переобучения
Переобучение (overfitting) возникает, когда модель слишком хорошо адаптируется к обучающим данным, включая шум и выбросы, что приводит к плохому обобщению на новых данных.

Способы контроля переобучения
Регуляризация: Включение дополнительных терминов в функцию потерь, таких как L1 или L2 регуляризация, чтобы наказать за слишком большие веса.
Раннее прекращение (early stopping): Прекращение обучения, когда производительность на валидационном наборе перестает улучшаться.
Dropout: Временное отключение случайных нейронов во время обучения, чтобы избежать зависимости между нейронами.
Увеличение данных (data augmentation): Искусственное увеличение количества данных путем создания измененных копий обучающих данных (например, повороты, сдвиги изображений).
Кросс-валидация: Разделение данных на несколько частей и обучение модели на различных подвыборках данных.
—

31. Понятие линейной регрессии. Средства пакета tidymodels для решения задачи регрессии. 

—
Линейная регрессия — это метод статистического анализа, который моделирует зависимость между зависимой переменной (целью) и одной или несколькими независимыми переменными (предикторами), используя линейные функции. Основная цель — найти линейное уравнение, которое наилучшим образом описывает зависимость

Средства пакета tidymodels для решения задачи регрессии
tidymodels — это набор пакетов в R для создания и сравнения моделей машинного обучения.

Основные пакеты и функции:
parsnip: Создание моделей. Функции linear_reg() и set_engine("lm") для линейной регрессии.
recipes: Предобработка данных. Функция recipe() для создания рецептов предобработки.
workflows: Объединение модели и рецепта в единый объект. Функция workflow().
tune: Настройка гиперпараметров. Функции tune_grid() и tune_bayes() для поиска оптимальных гиперпараметров.
yardstick: Оценка модели. Функции для расчета метрик, таких как rmse() и rsq().
—

32. Понятие кластеризации. Метод k-ближайших соседей.

—
Кластеризация — это метод анализа данных, целью которого является разделение множества объектов на группы (кластеры) таким образом, чтобы объекты в одном кластере были более похожи друг на друга, чем на объекты в других кластерах.

Метод k-ближайших соседей (k-NN)
k-NN — это метод классификации и регрессии, который использует близость объектов для прогнозирования метки или значения нового объекта.

Основные шаги метода:

Определение числа соседей (k): Задается количество ближайших соседей, которые будут учитываться.
Вычисление расстояний: Для нового объекта вычисляются расстояния до всех объектов обучающей выборки (обычно используется евклидово расстояние).
Поиск ближайших соседей: Выбираются k объектов с наименьшими расстояниями.
Прогнозирование:
Классификация: Определение класса по большинству среди соседей.
Регрессия: Вычисление среднего значения среди соседей.
Метод k-NN прост в реализации и часто используется для задач, где требуется высокая точность при небольших объемах данных.
—









ПРАКТИКА

1. Используя датасет yaqry_dataset.pqt из бакета arrow-datasets Yandex ObjectStorage с помощью Yandex Query подсчитать количество строк и столбцов в нем.

—
SELECT COUNT(*) FROM yaqry2dataset
—

2. Используя датасет yaqry_dataset.pqt из бакета arrow-datasets Yandex ObjectStorage с помощью Yandex Query, определите количество хостов внутренней сети, представленных в датасете. Известно, что IP адреса внутренней сети начинаются с октетов, принадлежащих интервалу [12-14]. 

—
SELECT COUNT(DISTINCT src) AS host_count
FROM yaqry2dataset
WHERE src REGEXP '(^1[2-4].)';
—

В SQL-запросе мы используем функцию COUNT(DISTINCT src) для подсчета количества уникальных значений столбца src в таблице yaqry2dataset, которые удовлетворяют условию фильтрации. В разделе WHERE мы используем оператор REGEXP для указания регулярного выражения. В данном случае мы ищем строки, где значение столбца src начинается с числа 1, а следующий символ является числом от 2 до 4. Знак ^ в регулярном выражении означает начало строки, а 2-4 означает любое число от 2 до 4.

3. Используя датасет yaqry_dataset.pqt из бакета arrow-datasets Yandex ObjectStorage с помощью Yandex DataLens, представить в виде столбчатой диаграммы соотношение входящего и исходящего трафика из внутреннего сетвого сегмента. Известно, что IP адреса внутренней сети начинаются с октетов, принадлежащих интервалу [12-14] 

—
IF (LEFT([src], 3) IN ('12.', '13.', '14.') AND LEFT([dst], 3) NOT IN ('12.', '13.', '14.'))
    THEN 'Исходящий трафик'
ELSEIF (LEFT([dst], 3) IN ('12.', '13.', '14.') AND LEFT([src], 3) NOT IN ('12.', '13.', '14.'))
    THEN 'Входящий трафик'
ELSE
    NULL
END
—

Используем функцию LEFT, чтобы получить первые три символа из столбцов src и dst и сравнить их со списком значений (‘12.’, ‘13.’, ‘14.’). Это позволяет нам определить, принадлежит ли источник или назначение к внутреннему трафику, и соответственно классифицировать его как “Исходящий трафик” или “Входящий трафик”. Если ни одно из условий не выполняется, результат будет NULL.

 4. Используя датасет yaqry_dataset.pqt из бакета arrow-datasets Yandex ObjectStorage с помощью Yandex DataLens, представить в виде круговой диаграммы (pie chart) соотношение входящего и исходящего трафика из внутреннего сетвого сегмента. Известно, что IP адреса внутренней сети начинаются с октетов, принадлежащих интервалу [12-14].
 
—
IF (([src] LIKE '12.%' OR [src] LIKE '13.%' OR [src] LIKE '14.%') AND ([dst] LIKE '12.%' OR [dst] LIKE '13.%' OR [dst] LIKE '14.%'))
  THEN 'Внутренний трафик'
ELSE
  'Внешний трафик'
END
—

Если значение в столбце src начинается с “12.”, “13.” или “14.” и значение в столбце dst также начинается с “12.”, “13.” или “14.”, то условие считается истинным и результатом будет “Внутренний трафик”.

Если условие не выполняется (значение в src и dst не начинается с “12.”, “13.” или “14.”), то результатом будет “Внешний трафик”.

Импортируйте датасеты в R. Датасет находится по адресу https://storage.yandexcloud.net/iamcth-data/P2_4_DNSLog.zip. Восстановить названия столбцов в датасете dns.log , основываясь на содержимом headers.csv , подготвить единый датафрейм. 
с 4ой строчки
id.orig_h,addr,The source IP address of the host that initiated the DNS query
id.orig_p,port,The source port of the host that initiated the DNS query
id.resp_h,addr,The destination IP address, typically the DNS server, that received the DNS query
id_resp_p,port,The destination port number, usually port 53 for DNS, on the server that received the DNS query
—
data = read.csv("dns.log", header = FALSE, sep="\t", encoding = "UTF-8")
header = read.csv("header.csv", encoding = "UTF-8", skip = 1, header = FALSE, sep = ',')$V1
colnames(data) = header
—

6. Импортируйте датасеты в R. Датасет находится по адресу https://storage.yandexcloud.net/iamcth-data/P2_4_DNSLog.zip. Найти топ-10 участников сети, проявляющих наибольшую сетевую активность.
с 4ой строчки
id.orig_h,addr,The source IP address of the host that initiated the DNS query
id.orig_p,port,The source port of the host that initiated the DNS query
id.resp_h,addr,The destination IP address, typically the DNS server, that received the DNS query
id_resp_p,port,The destination port number, usually port 53 for DNS, on the server that received the DNS query
—
data = read.csv("dns.log", header = FALSE, sep="\t", encoding = "UTF-8")
header = read.csv("header.csv", encoding = "UTF-8", skip = 1, header = FALSE, sep = ',')$V1
colnames(data) = header
top_activity <- data %>%
  group_by(ip=`id.orig_h `) %>%
  summarise(activity = n()) %>%
  arrange(desc(activity)) %>%
  head(10)
ip <- select(top_activity,ip)
ip
—

7. Используя R, определить самый высокогорный аэропорт в датасете nycflights13::airports. 
.
Присваиваем переменной alpine_airport результат работы функций: arrange(desc(alt)), которая сортирует строки в порядке убывания значения столбца alt (высота над уровнем моря); head(1), которая возвращает первую строку (то есть аэропорт с наибольшей высотой над уровнем моря) и выводим на экран.
—
library(nycflights13)
alpine_airport <- airports %>%
  arrange(desc(alt)) %>%
  head(1)
cat(alpine_airport$name)
—

8. Используя R, определить самый северный аэропорт в датасете nycflights13::airports.

Присваиваем переменной N_airports значение работы функций: arrange(desc(lat)), которая сортирует строки в порядке убывания результат столбца lat (широта); head(1), которая возвращает первую строку (то есть аэропорт с наибольшей широтой) и выводим результат работы на экран.
—
library(nycflights13)
N_airports <- airports %>%
  arrange(desc(lat)) %>%
  head(1)
cat(N_airports$name)
—

9. Используя данные, хранящиеся в бакете Yandex Object Storage arrowdatasets/tm_data.pqt с помощью R и пакета Arrow определите количество хостов внутренней сети, представленных в датасете. Известно, что IP адреса внутренней сети начинаются с октетов, принадлежащих интервалу [12-14]. 

—
library(arrow)
library(dplyr)
data <- arrow::read_parquet("https://storage.yandexcloud.net/arrow-datasets/tm_data.pqt")
leadkedData <- data %>%
  filter(grepl("^12\\.", src) | grepl("^13\\.", src) | grepl("^14\\.", src)) %>%
  summarise(pr_1 = n_distinct(src))
print(leadkedData)
—

Чтение данных из файла Parquet, расположенного по указанному URL, и сохранение этих данных в переменную data.
Создание нового фрейма данных leadkedData, фильтруем строки, у которых значение в столбце src начинается с "12.", "13." или "14.". создание таблицу, которая содержит один столбец pr_1, представляющий количество уникальных значений в столбце src после фильтрации.
grepl — это функция в языке программирования R, которая используется для поиска шаблонов в строках и возвращает логическое значение (TRUE или FALSE) для каждой строки в векторе. 

10. Используя данные, хранящиеся в бакете Yandex Object Storage arrowdatasets/tm_data.pqt с помощью R и СУБД DuckDB определите количество хостов внутренней сети, представленных в датасете. Известно, что IP адреса внутренней сети начинаются с октетов, принадлежащих интервалу [12-14].

—
library(duckdb)
library(dplyr)
library(tidyverse)
connection <- dbConnect(duckdb::duckdb(), dbdir = ":memory:")
dbExecute(conn = connection, "INSTALL httpfs; LOAD httpfs;")
PQF = "https://storage.yandexcloud.net/arrow-datasets/tm_data.pqt"
SQL <- "SELECT * FROM read_parquet([?])"
data <- dbGetQuery(connection, SQL, list(PQF))
leadkedData <- <- df %>% filter(grepl("^12\\.", src) | grepl("^13\\.", src) | grepl("^14\\.", src)) %>% summarise(pr_1 = n_distinct(src))
leadkedData
—
Создание подключения к DuckDB с базой данных, расположенной в памяти.
становка и загрузка расширения HTTPFS, которое позволяет работать с файлами по HTTP.
Сохранение URL к Parquet файлу в переменную PQF.
Создание SQL-запроса для чтения данных из Parquet файла.
Выполнение SQL-запроса и загрузка данных в переменную data.
ильтрация строк, у которых значение в столбце src начинается с "12.", "13." или "14.", и подсчет количества уникальных значений в столбце src после фильтрации.


